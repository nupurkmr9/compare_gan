/home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow_gan/python/contrib_utils.py:48: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow_gan/python/contrib_utils.py:80: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow_gan/python/losses/losses_impl.py:73: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.

WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow_gan/python/train.py:766: The name tf.losses.absolute_difference is deprecated. Please use tf.compat.v1.losses.absolute_difference instead.

WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow_gan/python/train.py:768: The name tf.losses.softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.softmax_cross_entropy instead.

WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow_gan/python/train.py:1037: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/gin/tf/utils.py:34: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.

WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/gin/tf/utils.py:34: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.

WARNING:tensorflow:From /home/nupkumar/compare_gan/compare_gan/architectures/abstract_arch.py:71: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

WARNING:tensorflow:From /home/nupkumar/compare_gan/compare_gan/gans/modular_gan.py:72: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/gin/tf/external_configurables.py:32: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.

WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/gin/tf/external_configurables.py:33: The name tf.train.inverse_time_decay is deprecated. Please use tf.compat.v1.train.inverse_time_decay instead.

WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/gin/tf/external_configurables.py:50: The name tf.losses.hinge_loss is deprecated. Please use tf.compat.v1.losses.hinge_loss instead.

WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/gin/tf/external_configurables.py:51: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.

I0920 19:51:40.912811 140175996602176 main.py:110] Gin config: ['../example_configs/biggan_imagenet128_aet.gin']
Gin bindings: []
I0920 19:51:40.923667 140175996602176 runner_lib.py:314] Running schedule 'continuous_eval' with options: {'use_tpu': False, 'batch_size': 2048, 'gan_class': <class 'compare_gan.gans.modular_gan.ModularGAN_Aux_Task_AET_v2'>, 'architecture': 'resnet_biggan_arch', 'training_steps': 250000, 'lambda': 1, 'consistency_ratio': 10, 'augment_w': 4, 'disc_iters': 2, 'z_dim': 120, 'gen_iters': 1, 'aux_net': 'aux_network_aet_v2'}
WARNING:tensorflow:From /home/nupkumar/compare_gan/compare_gan/gans/utils.py:28: The name tf.gfile.IsDirectory is deprecated. Please use tf.io.gfile.isdir instead.

W0920 19:51:40.923798 140175996602176 deprecation_wrapper.py:119] From /home/nupkumar/compare_gan/compare_gan/gans/utils.py:28: The name tf.gfile.IsDirectory is deprecated. Please use tf.io.gfile.isdir instead.

I0920 19:51:41.104869 140175996602176 datasets.py:103] ImageDatasetV2(name=imagenet_128, tfds_name=imagenet2012, resolution=128, colors=3, num_classes=1000, eval_test_samples=50000, seed=547)
WARNING:tensorflow:From /home/nupkumar/compare_gan/compare_gan/gans/modular_gan.py:296: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W0920 19:51:41.106296 140175996602176 deprecation_wrapper.py:119] From /home/nupkumar/compare_gan/compare_gan/gans/modular_gan.py:296: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

I0920 19:51:41.109892 140175996602176 modular_gan.py:320] Creating module for model gen with inputs {'z': <tf.Tensor 'z_for_eval:0' shape=(?, 120) dtype=float32>, 'labels': <tf.Tensor 'labels_for_eval:0' shape=(?,) dtype=int32>} and y=Tensor("one_hot:0", shape=(?, 1000), dtype=float32)
WARNING:tensorflow:From /home/nupkumar/compare_gan/compare_gan/architectures/abstract_arch.py:72: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0920 19:51:41.110478 140175996602176 deprecation_wrapper.py:119] From /home/nupkumar/compare_gan/compare_gan/architectures/abstract_arch.py:72: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

I0920 19:51:41.110680 140175996602176 resnet_biggan.py:236] [Generator] inputs are z=(?, 120), y=(?, 1000)
WARNING:tensorflow:From /home/nupkumar/compare_gan/compare_gan/architectures/arch_ops.py:543: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

W0920 19:51:41.110877 140175996602176 deprecation_wrapper.py:119] From /home/nupkumar/compare_gan/compare_gan/architectures/arch_ops.py:543: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

I0920 19:51:41.129035 140175996602176 resnet_biggan.py:262] [Generator] z0=(?, 20), z_per_block=['(?, 20)', '(?, 20)', '(?, 20)', '(?, 20)', '(?, 20)'], y_per_block=['(?, 148)', '(?, 148)', '(?, 148)', '(?, 148)', '(?, 148)']
I0920 19:51:41.510691 140175996602176 resnet_biggan.py:150] [Block] (?, 4, 4, 1536) (z=(?, 20), y=(?, 148)) -> (?, 8, 8, 1536)
I0920 19:51:41.865823 140175996602176 resnet_biggan.py:150] [Block] (?, 8, 8, 1536) (z=(?, 20), y=(?, 148)) -> (?, 16, 16, 768)
I0920 19:51:42.273605 140175996602176 resnet_biggan.py:150] [Block] (?, 16, 16, 768) (z=(?, 20), y=(?, 148)) -> (?, 32, 32, 384)
I0920 19:51:42.605411 140175996602176 resnet_biggan.py:150] [Block] (?, 32, 32, 384) (z=(?, 20), y=(?, 148)) -> (?, 64, 64, 192)
I0920 19:51:42.605621 140175996602176 resnet_biggan.py:289] [Generator] Applying non-local block to (?, 64, 64, 192)
WARNING:tensorflow:From /home/nupkumar/compare_gan/compare_gan/architectures/arch_ops.py:741: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
W0920 19:51:42.667212 140175996602176 deprecation.py:323] From /home/nupkumar/compare_gan/compare_gan/architectures/arch_ops.py:741: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0730160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0730160>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:51:42.707973 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0730160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0730160>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0676550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0676550>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:51:42.778397 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0676550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0676550>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:51:43.151735 140175996602176 resnet_biggan.py:150] [Block] (?, 64, 64, 192) (z=(?, 20), y=(?, 148)) -> (?, 128, 128, 96)
I0920 19:51:43.151952 140175996602176 resnet_biggan.py:294] [Generator] before final processing: (?, 128, 128, 96)
I0920 19:51:43.319743 140175996602176 resnet_biggan.py:300] [Generator] after final processing: (?, 128, 128, 3)
WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0920 19:51:43.322350 140175996602176 deprecation.py:323] From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0920 19:51:44.069887 140175996602176 resnet_biggan.py:236] [Generator] inputs are z=(?, 120), y=(?, 1000)
I0920 19:51:44.077969 140175996602176 resnet_biggan.py:262] [Generator] z0=(?, 20), z_per_block=['(?, 20)', '(?, 20)', '(?, 20)', '(?, 20)', '(?, 20)'], y_per_block=['(?, 148)', '(?, 148)', '(?, 148)', '(?, 148)', '(?, 148)']
I0920 19:51:44.305169 140175996602176 resnet_biggan.py:150] [Block] (?, 4, 4, 1536) (z=(?, 20), y=(?, 148)) -> (?, 8, 8, 1536)
I0920 19:51:44.507899 140175996602176 resnet_biggan.py:150] [Block] (?, 8, 8, 1536) (z=(?, 20), y=(?, 148)) -> (?, 16, 16, 768)
I0920 19:51:44.806339 140175996602176 resnet_biggan.py:150] [Block] (?, 16, 16, 768) (z=(?, 20), y=(?, 148)) -> (?, 32, 32, 384)
I0920 19:51:45.005446 140175996602176 resnet_biggan.py:150] [Block] (?, 32, 32, 384) (z=(?, 20), y=(?, 148)) -> (?, 64, 64, 192)
I0920 19:51:45.005687 140175996602176 resnet_biggan.py:289] [Generator] Applying non-local block to (?, 64, 64, 192)
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9afb9a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9afb9a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:51:45.077449 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9afb9a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9afb9a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9af71278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9af71278>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:51:45.134183 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9af71278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9af71278>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:51:45.356679 140175996602176 resnet_biggan.py:150] [Block] (?, 64, 64, 192) (z=(?, 20), y=(?, 148)) -> (?, 128, 128, 96)
I0920 19:51:45.356909 140175996602176 resnet_biggan.py:294] [Generator] before final processing: (?, 128, 128, 96)
I0920 19:51:45.407150 140175996602176 resnet_biggan.py:300] [Generator] after final processing: (?, 128, 128, 3)
WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow_hub/saved_model_lib.py:112: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.
W0920 19:51:45.409807 140175996602176 deprecation.py:323] From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow_hub/saved_model_lib.py:112: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.
I0920 19:51:45.894929 140175996602176 modular_gan.py:320] Creating module for model gen with inputs {'z': <tf.Tensor 'z_for_eval:0' shape=(8, 120) dtype=float32>, 'labels': <tf.Tensor 'labels_for_eval:0' shape=(8,) dtype=int32>} and y=Tensor("one_hot:0", shape=(8, 1000), dtype=float32)
I0920 19:51:45.895297 140175996602176 resnet_biggan.py:236] [Generator] inputs are z=(8, 120), y=(8, 1000)
I0920 19:51:45.912916 140175996602176 resnet_biggan.py:262] [Generator] z0=(8, 20), z_per_block=['(8, 20)', '(8, 20)', '(8, 20)', '(8, 20)', '(8, 20)'], y_per_block=['(8, 148)', '(8, 148)', '(8, 148)', '(8, 148)', '(8, 148)']
I0920 19:51:46.311104 140175996602176 resnet_biggan.py:150] [Block] (8, 4, 4, 1536) (z=(8, 20), y=(8, 148)) -> (8, 8, 8, 1536)
I0920 19:51:46.837337 140175996602176 resnet_biggan.py:150] [Block] (8, 8, 8, 1536) (z=(8, 20), y=(8, 148)) -> (8, 16, 16, 768)
I0920 19:51:47.163553 140175996602176 resnet_biggan.py:150] [Block] (8, 16, 16, 768) (z=(8, 20), y=(8, 148)) -> (8, 32, 32, 384)
I0920 19:51:47.486158 140175996602176 resnet_biggan.py:150] [Block] (8, 32, 32, 384) (z=(8, 20), y=(8, 148)) -> (8, 64, 64, 192)
I0920 19:51:47.486389 140175996602176 resnet_biggan.py:289] [Generator] Applying non-local block to (8, 64, 64, 192)
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0739fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0739fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:51:47.584936 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0739fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0739fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0a01d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0a01d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:51:47.655837 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0a01d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0a01d68>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:51:48.097534 140175996602176 resnet_biggan.py:150] [Block] (8, 64, 64, 192) (z=(8, 20), y=(8, 148)) -> (8, 128, 128, 96)
I0920 19:51:48.097758 140175996602176 resnet_biggan.py:294] [Generator] before final processing: (8, 128, 128, 96)
I0920 19:51:48.177493 140175996602176 resnet_biggan.py:300] [Generator] after final processing: (8, 128, 128, 3)
I0920 19:51:48.939295 140175996602176 resnet_biggan.py:236] [Generator] inputs are z=(8, 120), y=(8, 1000)
I0920 19:51:48.947058 140175996602176 resnet_biggan.py:262] [Generator] z0=(8, 20), z_per_block=['(8, 20)', '(8, 20)', '(8, 20)', '(8, 20)', '(8, 20)'], y_per_block=['(8, 148)', '(8, 148)', '(8, 148)', '(8, 148)', '(8, 148)']
I0920 19:51:49.252252 140175996602176 resnet_biggan.py:150] [Block] (8, 4, 4, 1536) (z=(8, 20), y=(8, 148)) -> (8, 8, 8, 1536)
I0920 19:51:49.439204 140175996602176 resnet_biggan.py:150] [Block] (8, 8, 8, 1536) (z=(8, 20), y=(8, 148)) -> (8, 16, 16, 768)
I0920 19:51:49.642934 140175996602176 resnet_biggan.py:150] [Block] (8, 16, 16, 768) (z=(8, 20), y=(8, 148)) -> (8, 32, 32, 384)
I0920 19:51:49.859467 140175996602176 resnet_biggan.py:150] [Block] (8, 32, 32, 384) (z=(8, 20), y=(8, 148)) -> (8, 64, 64, 192)
I0920 19:51:49.859725 140175996602176 resnet_biggan.py:289] [Generator] Applying non-local block to (8, 64, 64, 192)
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9bd12898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9bd12898>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:51:49.942589 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9bd12898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9bd12898>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9bd38f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9bd38f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:51:50.007975 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9bd38f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9bd38f98>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:51:50.219239 140175996602176 resnet_biggan.py:150] [Block] (8, 64, 64, 192) (z=(8, 20), y=(8, 148)) -> (8, 128, 128, 96)
I0920 19:51:50.219496 140175996602176 resnet_biggan.py:294] [Generator] before final processing: (8, 128, 128, 96)
I0920 19:51:50.264087 140175996602176 resnet_biggan.py:300] [Generator] after final processing: (8, 128, 128, 3)
I0920 19:51:50.808071 140175996602176 modular_gan.py:320] Creating module for model gen with inputs {'z': <tf.Tensor 'z_for_eval:0' shape=(16, 120) dtype=float32>, 'labels': <tf.Tensor 'labels_for_eval:0' shape=(16,) dtype=int32>} and y=Tensor("one_hot:0", shape=(16, 1000), dtype=float32)
I0920 19:51:50.808436 140175996602176 resnet_biggan.py:236] [Generator] inputs are z=(16, 120), y=(16, 1000)
I0920 19:51:50.826488 140175996602176 resnet_biggan.py:262] [Generator] z0=(16, 20), z_per_block=['(16, 20)', '(16, 20)', '(16, 20)', '(16, 20)', '(16, 20)'], y_per_block=['(16, 148)', '(16, 148)', '(16, 148)', '(16, 148)', '(16, 148)']
I0920 19:51:51.223497 140175996602176 resnet_biggan.py:150] [Block] (16, 4, 4, 1536) (z=(16, 20), y=(16, 148)) -> (16, 8, 8, 1536)
I0920 19:51:51.584208 140175996602176 resnet_biggan.py:150] [Block] (16, 8, 8, 1536) (z=(16, 20), y=(16, 148)) -> (16, 16, 16, 768)
I0920 19:51:51.936692 140175996602176 resnet_biggan.py:150] [Block] (16, 16, 16, 768) (z=(16, 20), y=(16, 148)) -> (16, 32, 32, 384)
I0920 19:51:52.265372 140175996602176 resnet_biggan.py:150] [Block] (16, 32, 32, 384) (z=(16, 20), y=(16, 148)) -> (16, 64, 64, 192)
I0920 19:51:52.265584 140175996602176 resnet_biggan.py:289] [Generator] Applying non-local block to (16, 64, 64, 192)
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c99865fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c99865fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:51:52.364930 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c99865fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c99865fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c997cd908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c997cd908>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:51:52.434887 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c997cd908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c997cd908>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:51:52.997452 140175996602176 resnet_biggan.py:150] [Block] (16, 64, 64, 192) (z=(16, 20), y=(16, 148)) -> (16, 128, 128, 96)
I0920 19:51:52.997686 140175996602176 resnet_biggan.py:294] [Generator] before final processing: (16, 128, 128, 96)
I0920 19:51:53.082543 140175996602176 resnet_biggan.py:300] [Generator] after final processing: (16, 128, 128, 3)
I0920 19:51:53.879869 140175996602176 resnet_biggan.py:236] [Generator] inputs are z=(16, 120), y=(16, 1000)
I0920 19:51:53.888276 140175996602176 resnet_biggan.py:262] [Generator] z0=(16, 20), z_per_block=['(16, 20)', '(16, 20)', '(16, 20)', '(16, 20)', '(16, 20)'], y_per_block=['(16, 148)', '(16, 148)', '(16, 148)', '(16, 148)', '(16, 148)']
I0920 19:51:54.210376 140175996602176 resnet_biggan.py:150] [Block] (16, 4, 4, 1536) (z=(16, 20), y=(16, 148)) -> (16, 8, 8, 1536)
I0920 19:51:54.401106 140175996602176 resnet_biggan.py:150] [Block] (16, 8, 8, 1536) (z=(16, 20), y=(16, 148)) -> (16, 16, 16, 768)
I0920 19:51:54.587487 140175996602176 resnet_biggan.py:150] [Block] (16, 16, 16, 768) (z=(16, 20), y=(16, 148)) -> (16, 32, 32, 384)
I0920 19:51:54.776826 140175996602176 resnet_biggan.py:150] [Block] (16, 32, 32, 384) (z=(16, 20), y=(16, 148)) -> (16, 64, 64, 192)
I0920 19:51:54.777066 140175996602176 resnet_biggan.py:289] [Generator] Applying non-local block to (16, 64, 64, 192)
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca13f0ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca13f0ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:51:54.847668 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca13f0ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca13f0ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca13a94a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca13a94a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:51:54.905159 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca13a94a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca13a94a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:51:55.116501 140175996602176 resnet_biggan.py:150] [Block] (16, 64, 64, 192) (z=(16, 20), y=(16, 148)) -> (16, 128, 128, 96)
I0920 19:51:55.116745 140175996602176 resnet_biggan.py:294] [Generator] before final processing: (16, 128, 128, 96)
I0920 19:51:55.162373 140175996602176 resnet_biggan.py:300] [Generator] after final processing: (16, 128, 128, 3)
I0920 19:51:55.605441 140175996602176 modular_gan.py:320] Creating module for model gen with inputs {'z': <tf.Tensor 'z_for_eval:0' shape=(32, 120) dtype=float32>, 'labels': <tf.Tensor 'labels_for_eval:0' shape=(32,) dtype=int32>} and y=Tensor("one_hot:0", shape=(32, 1000), dtype=float32)
I0920 19:51:55.605783 140175996602176 resnet_biggan.py:236] [Generator] inputs are z=(32, 120), y=(32, 1000)
I0920 19:51:55.623690 140175996602176 resnet_biggan.py:262] [Generator] z0=(32, 20), z_per_block=['(32, 20)', '(32, 20)', '(32, 20)', '(32, 20)', '(32, 20)'], y_per_block=['(32, 148)', '(32, 148)', '(32, 148)', '(32, 148)', '(32, 148)']
I0920 19:51:56.205512 140175996602176 resnet_biggan.py:150] [Block] (32, 4, 4, 1536) (z=(32, 20), y=(32, 148)) -> (32, 8, 8, 1536)
I0920 19:51:56.537797 140175996602176 resnet_biggan.py:150] [Block] (32, 8, 8, 1536) (z=(32, 20), y=(32, 148)) -> (32, 16, 16, 768)
I0920 19:51:56.866721 140175996602176 resnet_biggan.py:150] [Block] (32, 16, 16, 768) (z=(32, 20), y=(32, 148)) -> (32, 32, 32, 384)
I0920 19:51:57.280160 140175996602176 resnet_biggan.py:150] [Block] (32, 32, 32, 384) (z=(32, 20), y=(32, 148)) -> (32, 64, 64, 192)
I0920 19:51:57.280390 140175996602176 resnet_biggan.py:289] [Generator] Applying non-local block to (32, 64, 64, 192)
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca1428f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca1428f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:51:57.379466 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca1428f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca1428f60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca110bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca110bfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:51:57.455756 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca110bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca110bfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:51:57.822350 140175996602176 resnet_biggan.py:150] [Block] (32, 64, 64, 192) (z=(32, 20), y=(32, 148)) -> (32, 128, 128, 96)
I0920 19:51:57.822580 140175996602176 resnet_biggan.py:294] [Generator] before final processing: (32, 128, 128, 96)
I0920 19:51:57.906981 140175996602176 resnet_biggan.py:300] [Generator] after final processing: (32, 128, 128, 3)
I0920 19:51:58.774510 140175996602176 resnet_biggan.py:236] [Generator] inputs are z=(32, 120), y=(32, 1000)
I0920 19:51:58.782599 140175996602176 resnet_biggan.py:262] [Generator] z0=(32, 20), z_per_block=['(32, 20)', '(32, 20)', '(32, 20)', '(32, 20)', '(32, 20)'], y_per_block=['(32, 148)', '(32, 148)', '(32, 148)', '(32, 148)', '(32, 148)']
I0920 19:51:58.998776 140175996602176 resnet_biggan.py:150] [Block] (32, 4, 4, 1536) (z=(32, 20), y=(32, 148)) -> (32, 8, 8, 1536)
I0920 19:51:59.187346 140175996602176 resnet_biggan.py:150] [Block] (32, 8, 8, 1536) (z=(32, 20), y=(32, 148)) -> (32, 16, 16, 768)
I0920 19:51:59.373620 140175996602176 resnet_biggan.py:150] [Block] (32, 16, 16, 768) (z=(32, 20), y=(32, 148)) -> (32, 32, 32, 384)
I0920 19:51:59.560578 140175996602176 resnet_biggan.py:150] [Block] (32, 32, 32, 384) (z=(32, 20), y=(32, 148)) -> (32, 64, 64, 192)
I0920 19:51:59.560804 140175996602176 resnet_biggan.py:289] [Generator] Applying non-local block to (32, 64, 64, 192)
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9b1d1ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9b1d1ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:51:59.631437 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9b1d1ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9b1d1ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9b18a4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9b18a4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:51:59.687679 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9b18a4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9b18a4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:52:00.004400 140175996602176 resnet_biggan.py:150] [Block] (32, 64, 64, 192) (z=(32, 20), y=(32, 148)) -> (32, 128, 128, 96)
I0920 19:52:00.004680 140175996602176 resnet_biggan.py:294] [Generator] before final processing: (32, 128, 128, 96)
I0920 19:52:00.057693 140175996602176 resnet_biggan.py:300] [Generator] after final processing: (32, 128, 128, 3)
I0920 19:52:00.510479 140175996602176 modular_gan.py:320] Creating module for model gen with inputs {'z': <tf.Tensor 'z_for_eval:0' shape=(64, 120) dtype=float32>, 'labels': <tf.Tensor 'labels_for_eval:0' shape=(64,) dtype=int32>} and y=Tensor("one_hot:0", shape=(64, 1000), dtype=float32)
I0920 19:52:00.510827 140175996602176 resnet_biggan.py:236] [Generator] inputs are z=(64, 120), y=(64, 1000)
I0920 19:52:00.528740 140175996602176 resnet_biggan.py:262] [Generator] z0=(64, 20), z_per_block=['(64, 20)', '(64, 20)', '(64, 20)', '(64, 20)', '(64, 20)'], y_per_block=['(64, 148)', '(64, 148)', '(64, 148)', '(64, 148)', '(64, 148)']
I0920 19:52:00.907235 140175996602176 resnet_biggan.py:150] [Block] (64, 4, 4, 1536) (z=(64, 20), y=(64, 148)) -> (64, 8, 8, 1536)
I0920 19:52:01.242038 140175996602176 resnet_biggan.py:150] [Block] (64, 8, 8, 1536) (z=(64, 20), y=(64, 148)) -> (64, 16, 16, 768)
I0920 19:52:01.773534 140175996602176 resnet_biggan.py:150] [Block] (64, 16, 16, 768) (z=(64, 20), y=(64, 148)) -> (64, 32, 32, 384)
I0920 19:52:02.107033 140175996602176 resnet_biggan.py:150] [Block] (64, 32, 32, 384) (z=(64, 20), y=(64, 148)) -> (64, 64, 64, 192)
I0920 19:52:02.107279 140175996602176 resnet_biggan.py:289] [Generator] Applying non-local block to (64, 64, 64, 192)
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0aabf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0aabf60>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:02.209261 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0aabf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0aabf60>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0bd7fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0bd7fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:02.279507 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0bd7fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0bd7fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:52:02.642521 140175996602176 resnet_biggan.py:150] [Block] (64, 64, 64, 192) (z=(64, 20), y=(64, 148)) -> (64, 128, 128, 96)
I0920 19:52:02.642740 140175996602176 resnet_biggan.py:294] [Generator] before final processing: (64, 128, 128, 96)
I0920 19:52:02.721746 140175996602176 resnet_biggan.py:300] [Generator] after final processing: (64, 128, 128, 3)
I0920 19:52:03.558041 140175996602176 resnet_biggan.py:236] [Generator] inputs are z=(64, 120), y=(64, 1000)
I0920 19:52:03.565930 140175996602176 resnet_biggan.py:262] [Generator] z0=(64, 20), z_per_block=['(64, 20)', '(64, 20)', '(64, 20)', '(64, 20)', '(64, 20)'], y_per_block=['(64, 148)', '(64, 148)', '(64, 148)', '(64, 148)', '(64, 148)']
I0920 19:52:03.773059 140175996602176 resnet_biggan.py:150] [Block] (64, 4, 4, 1536) (z=(64, 20), y=(64, 148)) -> (64, 8, 8, 1536)
I0920 19:52:03.959887 140175996602176 resnet_biggan.py:150] [Block] (64, 8, 8, 1536) (z=(64, 20), y=(64, 148)) -> (64, 16, 16, 768)
I0920 19:52:04.158000 140175996602176 resnet_biggan.py:150] [Block] (64, 16, 16, 768) (z=(64, 20), y=(64, 148)) -> (64, 32, 32, 384)
I0920 19:52:04.456159 140175996602176 resnet_biggan.py:150] [Block] (64, 32, 32, 384) (z=(64, 20), y=(64, 148)) -> (64, 64, 64, 192)
I0920 19:52:04.456410 140175996602176 resnet_biggan.py:289] [Generator] Applying non-local block to (64, 64, 64, 192)
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca10b0ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca10b0ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:04.528071 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca10b0ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca10b0ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca10694a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca10694a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:04.584055 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca10694a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca10694a8>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:52:04.801967 140175996602176 resnet_biggan.py:150] [Block] (64, 64, 64, 192) (z=(64, 20), y=(64, 148)) -> (64, 128, 128, 96)
I0920 19:52:04.802187 140175996602176 resnet_biggan.py:294] [Generator] before final processing: (64, 128, 128, 96)
I0920 19:52:04.848330 140175996602176 resnet_biggan.py:300] [Generator] after final processing: (64, 128, 128, 3)
I0920 19:52:05.342646 140175996602176 modular_gan.py:320] Creating module for model gen with inputs {'z': <tf.Tensor 'z_for_eval:0' shape=(?, 120) dtype=float32>, 'labels': <tf.Tensor 'labels_for_eval:0' shape=(?,) dtype=int32>} and y=Tensor("one_hot:0", shape=(?, 1000), dtype=float32)
I0920 19:52:05.343038 140175996602176 resnet_biggan.py:236] [Generator] inputs are z=(?, 120), y=(?, 1000)
I0920 19:52:05.363912 140175996602176 resnet_biggan.py:262] [Generator] z0=(?, 20), z_per_block=['(?, 20)', '(?, 20)', '(?, 20)', '(?, 20)', '(?, 20)'], y_per_block=['(?, 148)', '(?, 148)', '(?, 148)', '(?, 148)', '(?, 148)']
I0920 19:52:05.765882 140175996602176 resnet_biggan.py:150] [Block] (?, 4, 4, 1536) (z=(?, 20), y=(?, 148)) -> (?, 8, 8, 1536)
I0920 19:52:06.104398 140175996602176 resnet_biggan.py:150] [Block] (?, 8, 8, 1536) (z=(?, 20), y=(?, 148)) -> (?, 16, 16, 768)
I0920 19:52:06.618826 140175996602176 resnet_biggan.py:150] [Block] (?, 16, 16, 768) (z=(?, 20), y=(?, 148)) -> (?, 32, 32, 384)
I0920 19:52:06.943521 140175996602176 resnet_biggan.py:150] [Block] (?, 32, 32, 384) (z=(?, 20), y=(?, 148)) -> (?, 64, 64, 192)
I0920 19:52:06.943741 140175996602176 resnet_biggan.py:289] [Generator] Applying non-local block to (?, 64, 64, 192)
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0793c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0793c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:07.048026 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0793c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0793c88>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9973e7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9973e7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:07.120831 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9973e7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9973e7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:52:07.575330 140175996602176 resnet_biggan.py:150] [Block] (?, 64, 64, 192) (z=(?, 20), y=(?, 148)) -> (?, 128, 128, 96)
I0920 19:52:07.575551 140175996602176 resnet_biggan.py:294] [Generator] before final processing: (?, 128, 128, 96)
I0920 19:52:07.658080 140175996602176 resnet_biggan.py:300] [Generator] after final processing: (?, 128, 128, 3)
I0920 19:52:08.392352 140175996602176 resnet_biggan.py:236] [Generator] inputs are z=(?, 120), y=(?, 1000)
I0920 19:52:08.399897 140175996602176 resnet_biggan.py:262] [Generator] z0=(?, 20), z_per_block=['(?, 20)', '(?, 20)', '(?, 20)', '(?, 20)', '(?, 20)'], y_per_block=['(?, 148)', '(?, 148)', '(?, 148)', '(?, 148)', '(?, 148)']
I0920 19:52:08.604464 140175996602176 resnet_biggan.py:150] [Block] (?, 4, 4, 1536) (z=(?, 20), y=(?, 148)) -> (?, 8, 8, 1536)
I0920 19:52:08.888135 140175996602176 resnet_biggan.py:150] [Block] (?, 8, 8, 1536) (z=(?, 20), y=(?, 148)) -> (?, 16, 16, 768)
I0920 19:52:09.079362 140175996602176 resnet_biggan.py:150] [Block] (?, 16, 16, 768) (z=(?, 20), y=(?, 148)) -> (?, 32, 32, 384)
I0920 19:52:09.266287 140175996602176 resnet_biggan.py:150] [Block] (?, 32, 32, 384) (z=(?, 20), y=(?, 148)) -> (?, 64, 64, 192)
I0920 19:52:09.266544 140175996602176 resnet_biggan.py:289] [Generator] Applying non-local block to (?, 64, 64, 192)
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0276a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0276a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:09.336221 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0276a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0276a90>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca01f0470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca01f0470>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:09.390589 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca01f0470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca01f0470>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:52:09.595756 140175996602176 resnet_biggan.py:150] [Block] (?, 64, 64, 192) (z=(?, 20), y=(?, 148)) -> (?, 128, 128, 96)
I0920 19:52:09.595978 140175996602176 resnet_biggan.py:294] [Generator] before final processing: (?, 128, 128, 96)
I0920 19:52:09.642764 140175996602176 resnet_biggan.py:300] [Generator] after final processing: (?, 128, 128, 3)
I0920 19:52:10.122311 140175996602176 modular_gan.py:320] Creating module for model disc with inputs {'images': <tf.Tensor 'images_for_eval:0' shape=(8, 128, 128, 3) dtype=float32>, 'labels': <tf.Tensor 'labels_for_eval:0' shape=(8,) dtype=int32>} and y=Tensor("one_hot:0", shape=(8, 1000), dtype=float32)
I0920 19:52:10.123030 140175996602176 resnet_biggan.py:379] [Discriminator] inputs are x=(8, 128, 128, 3), y=(8, 1000)
I0920 19:52:10.240411 140175996602176 resnet_biggan.py:150] [Block] (8, 128, 128, 3) (z=None, y=(8, 1000)) -> (8, 64, 64, 96)
I0920 19:52:10.240604 140175996602176 resnet_biggan.py:398] [Discriminator] Applying non-local block to (8, 64, 64, 96)
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9aa6c940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9aa6c940>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:10.338127 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9aa6c940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9aa6c940>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9a817198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9a817198>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:10.405666 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9a817198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9a817198>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:52:10.750360 140175996602176 resnet_biggan.py:150] [Block] (8, 64, 64, 96) (z=None, y=(8, 1000)) -> (8, 32, 32, 192)
I0920 19:52:10.862428 140175996602176 resnet_biggan.py:150] [Block] (8, 32, 32, 192) (z=None, y=(8, 1000)) -> (8, 16, 16, 384)
I0920 19:52:10.973959 140175996602176 resnet_biggan.py:150] [Block] (8, 16, 16, 384) (z=None, y=(8, 1000)) -> (8, 8, 8, 768)
I0920 19:52:11.084148 140175996602176 resnet_biggan.py:150] [Block] (8, 8, 8, 768) (z=None, y=(8, 1000)) -> (8, 4, 4, 1536)
I0920 19:52:11.154126 140175996602176 resnet_biggan.py:150] [Block] (8, 4, 4, 1536) (z=None, y=(8, 1000)) -> (8, 4, 4, 1536)
I0920 19:52:11.154307 140175996602176 resnet_biggan.py:403] [Discriminator] before final processing: (8, 4, 4, 1536)
I0920 19:52:11.189803 140175996602176 resnet_biggan.py:407] [Discriminator] after final processing: (8, 4, 4, 1536)
WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0920 19:52:11.190157 140175996602176 deprecation.py:506] From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
I0920 19:52:11.217491 140175996602176 resnet_biggan.py:422] [Discriminator] embedded_y for projection: (8, 1536)
I0920 19:52:11.334942 140175996602176 modular_gan.py:320] Creating module for model disc with inputs {'images': <tf.Tensor 'images_for_eval:0' shape=(16, 128, 128, 3) dtype=float32>, 'labels': <tf.Tensor 'labels_for_eval:0' shape=(16,) dtype=int32>} and y=Tensor("one_hot:0", shape=(16, 1000), dtype=float32)
I0920 19:52:11.335295 140175996602176 resnet_biggan.py:379] [Discriminator] inputs are x=(16, 128, 128, 3), y=(16, 1000)
I0920 19:52:11.448179 140175996602176 resnet_biggan.py:150] [Block] (16, 128, 128, 3) (z=None, y=(16, 1000)) -> (16, 64, 64, 96)
I0920 19:52:11.448360 140175996602176 resnet_biggan.py:398] [Discriminator] Applying non-local block to (16, 64, 64, 96)
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9bfde320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9bfde320>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:11.543797 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9bfde320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9bfde320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0b2c588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0b2c588>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:11.610960 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0b2c588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0b2c588>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:52:11.759807 140175996602176 resnet_biggan.py:150] [Block] (16, 64, 64, 96) (z=None, y=(16, 1000)) -> (16, 32, 32, 192)
I0920 19:52:11.868865 140175996602176 resnet_biggan.py:150] [Block] (16, 32, 32, 192) (z=None, y=(16, 1000)) -> (16, 16, 16, 384)
I0920 19:52:12.082237 140175996602176 resnet_biggan.py:150] [Block] (16, 16, 16, 384) (z=None, y=(16, 1000)) -> (16, 8, 8, 768)
I0920 19:52:12.193633 140175996602176 resnet_biggan.py:150] [Block] (16, 8, 8, 768) (z=None, y=(16, 1000)) -> (16, 4, 4, 1536)
I0920 19:52:12.264675 140175996602176 resnet_biggan.py:150] [Block] (16, 4, 4, 1536) (z=None, y=(16, 1000)) -> (16, 4, 4, 1536)
I0920 19:52:12.264854 140175996602176 resnet_biggan.py:403] [Discriminator] before final processing: (16, 4, 4, 1536)
I0920 19:52:12.299890 140175996602176 resnet_biggan.py:407] [Discriminator] after final processing: (16, 4, 4, 1536)
I0920 19:52:12.327074 140175996602176 resnet_biggan.py:422] [Discriminator] embedded_y for projection: (16, 1536)
I0920 19:52:12.442857 140175996602176 modular_gan.py:320] Creating module for model disc with inputs {'images': <tf.Tensor 'images_for_eval:0' shape=(32, 128, 128, 3) dtype=float32>, 'labels': <tf.Tensor 'labels_for_eval:0' shape=(32,) dtype=int32>} and y=Tensor("one_hot:0", shape=(32, 1000), dtype=float32)
I0920 19:52:12.443177 140175996602176 resnet_biggan.py:379] [Discriminator] inputs are x=(32, 128, 128, 3), y=(32, 1000)
I0920 19:52:12.554840 140175996602176 resnet_biggan.py:150] [Block] (32, 128, 128, 3) (z=None, y=(32, 1000)) -> (32, 64, 64, 96)
I0920 19:52:12.555016 140175996602176 resnet_biggan.py:398] [Discriminator] Applying non-local block to (32, 64, 64, 96)
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9a637cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9a637cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:12.650721 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9a637cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9a637cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0103588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0103588>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:12.718146 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0103588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0103588>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:52:12.867258 140175996602176 resnet_biggan.py:150] [Block] (32, 64, 64, 96) (z=None, y=(32, 1000)) -> (32, 32, 32, 192)
I0920 19:52:12.977344 140175996602176 resnet_biggan.py:150] [Block] (32, 32, 32, 192) (z=None, y=(32, 1000)) -> (32, 16, 16, 384)
I0920 19:52:13.095046 140175996602176 resnet_biggan.py:150] [Block] (32, 16, 16, 384) (z=None, y=(32, 1000)) -> (32, 8, 8, 768)
I0920 19:52:13.315186 140175996602176 resnet_biggan.py:150] [Block] (32, 8, 8, 768) (z=None, y=(32, 1000)) -> (32, 4, 4, 1536)
I0920 19:52:13.389547 140175996602176 resnet_biggan.py:150] [Block] (32, 4, 4, 1536) (z=None, y=(32, 1000)) -> (32, 4, 4, 1536)
I0920 19:52:13.389739 140175996602176 resnet_biggan.py:403] [Discriminator] before final processing: (32, 4, 4, 1536)
I0920 19:52:13.428267 140175996602176 resnet_biggan.py:407] [Discriminator] after final processing: (32, 4, 4, 1536)
I0920 19:52:13.457930 140175996602176 resnet_biggan.py:422] [Discriminator] embedded_y for projection: (32, 1536)
I0920 19:52:13.590108 140175996602176 modular_gan.py:320] Creating module for model disc with inputs {'images': <tf.Tensor 'images_for_eval:0' shape=(64, 128, 128, 3) dtype=float32>, 'labels': <tf.Tensor 'labels_for_eval:0' shape=(64,) dtype=int32>} and y=Tensor("one_hot:0", shape=(64, 1000), dtype=float32)
I0920 19:52:13.590458 140175996602176 resnet_biggan.py:379] [Discriminator] inputs are x=(64, 128, 128, 3), y=(64, 1000)
I0920 19:52:13.710605 140175996602176 resnet_biggan.py:150] [Block] (64, 128, 128, 3) (z=None, y=(64, 1000)) -> (64, 64, 64, 96)
I0920 19:52:13.710780 140175996602176 resnet_biggan.py:398] [Discriminator] Applying non-local block to (64, 64, 64, 96)
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9aba7320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9aba7320>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:13.810585 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9aba7320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7c9aba7320>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0249588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0249588>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:13.880783 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0249588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca0249588>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:52:14.029573 140175996602176 resnet_biggan.py:150] [Block] (64, 64, 64, 96) (z=None, y=(64, 1000)) -> (64, 32, 32, 192)
I0920 19:52:14.137926 140175996602176 resnet_biggan.py:150] [Block] (64, 32, 32, 192) (z=None, y=(64, 1000)) -> (64, 16, 16, 384)
I0920 19:52:14.245847 140175996602176 resnet_biggan.py:150] [Block] (64, 16, 16, 384) (z=None, y=(64, 1000)) -> (64, 8, 8, 768)
I0920 19:52:14.352818 140175996602176 resnet_biggan.py:150] [Block] (64, 8, 8, 768) (z=None, y=(64, 1000)) -> (64, 4, 4, 1536)
I0920 19:52:14.520628 140175996602176 resnet_biggan.py:150] [Block] (64, 4, 4, 1536) (z=None, y=(64, 1000)) -> (64, 4, 4, 1536)
I0920 19:52:14.520823 140175996602176 resnet_biggan.py:403] [Discriminator] before final processing: (64, 4, 4, 1536)
I0920 19:52:14.555295 140175996602176 resnet_biggan.py:407] [Discriminator] after final processing: (64, 4, 4, 1536)
I0920 19:52:14.583043 140175996602176 resnet_biggan.py:422] [Discriminator] embedded_y for projection: (64, 1536)
I0920 19:52:14.701349 140175996602176 modular_gan.py:320] Creating module for model disc with inputs {'images': <tf.Tensor 'images_for_eval:0' shape=(?, 128, 128, 3) dtype=float32>, 'labels': <tf.Tensor 'labels_for_eval:0' shape=(?,) dtype=int32>} and y=Tensor("one_hot:0", shape=(?, 1000), dtype=float32)
I0920 19:52:14.701663 140175996602176 resnet_biggan.py:379] [Discriminator] inputs are x=(?, 128, 128, 3), y=(?, 1000)
I0920 19:52:14.812217 140175996602176 resnet_biggan.py:150] [Block] (?, 128, 128, 3) (z=None, y=(?, 1000)) -> (?, 64, 64, 96)
I0920 19:52:14.812393 140175996602176 resnet_biggan.py:398] [Discriminator] Applying non-local block to (?, 64, 64, 96)
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca069f668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca069f668>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:14.913015 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca069f668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca069f668>>: AssertionError: Bad argument number for Name: 3, expecting 4
WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca1644320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca1644320>>: AssertionError: Bad argument number for Name: 3, expecting 4
W0920 19:52:14.981317 140175996602176 ag_logging.py:145] Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca1644320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7f7ca1644320>>: AssertionError: Bad argument number for Name: 3, expecting 4
I0920 19:52:15.141432 140175996602176 resnet_biggan.py:150] [Block] (?, 64, 64, 96) (z=None, y=(?, 1000)) -> (?, 32, 32, 192)
I0920 19:52:15.259486 140175996602176 resnet_biggan.py:150] [Block] (?, 32, 32, 192) (z=None, y=(?, 1000)) -> (?, 16, 16, 384)
I0920 19:52:15.376483 140175996602176 resnet_biggan.py:150] [Block] (?, 16, 16, 384) (z=None, y=(?, 1000)) -> (?, 8, 8, 768)
I0920 19:52:15.493612 140175996602176 resnet_biggan.py:150] [Block] (?, 8, 8, 768) (z=None, y=(?, 1000)) -> (?, 4, 4, 1536)
I0920 19:52:15.568681 140175996602176 resnet_biggan.py:150] [Block] (?, 4, 4, 1536) (z=None, y=(?, 1000)) -> (?, 4, 4, 1536)
I0920 19:52:15.568934 140175996602176 resnet_biggan.py:403] [Discriminator] before final processing: (?, 4, 4, 1536)
I0920 19:52:15.607711 140175996602176 resnet_biggan.py:407] [Discriminator] after final processing: (?, 4, 4, 1536)
I0920 19:52:15.637450 140175996602176 resnet_biggan.py:422] [Discriminator] embedded_y for projection: (?, 1536)
I0920 19:52:15.862465 140175996602176 runner_lib.py:265] eval_tasks: [<compare_gan.metrics.inception_score.InceptionScoreTask object at 0x7f7ca25c9d68>, <compare_gan.metrics.fid_score.FIDScoreTask object at 0x7f7ca256d8d0>]
I0920 19:52:15.862646 140175996602176 runner_lib.py:157] Looking for checkpoints in gs://fewshot_gan/model_imagenet_aet_bce0
I0920 19:52:16.892865 140175996602176 runner_lib.py:177] Found checkpoints: {'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-45000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-25000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-15000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-30000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-0', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-20000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-40000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-50000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-10000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-35000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-5000'}
Evaluated checkpoints: set()
Unevaluated checkpoints: ['gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-15000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-20000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-25000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-30000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-35000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-40000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-45000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-50000']
I0920 19:52:17.256862 140175996602176 datasets.py:103] ImageDatasetV2(name=imagenet_128, tfds_name=imagenet2012, resolution=128, colors=3, num_classes=1000, eval_test_samples=50000, seed=547)
2020-09-20 19:52:17.258610: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2020-09-20 19:52:17.274878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 19:52:17.275597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 19:52:17.275792: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 19:52:17.277019: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 19:52:17.278234: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 19:52:17.278524: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 19:52:17.280237: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 19:52:17.281495: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 19:52:17.285105: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 19:52:17.285208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 19:52:17.285944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 19:52:17.286606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 19:52:17.286899: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-09-20 19:52:17.498123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 19:52:17.498953: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8f4afd0 executing computations on platform CUDA. Devices:
2020-09-20 19:52:17.498979: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
2020-09-20 19:52:17.501982: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz
2020-09-20 19:52:17.502586: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xcc37bc0 executing computations on platform Host. Devices:
2020-09-20 19:52:17.502611: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2020-09-20 19:52:17.502776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 19:52:17.503518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 19:52:17.503585: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 19:52:17.503599: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 19:52:17.503610: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 19:52:17.503622: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 19:52:17.503643: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 19:52:17.503654: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 19:52:17.503665: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 19:52:17.503728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 19:52:17.504429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 19:52:17.505077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 19:52:17.505119: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 19:52:17.506293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 19:52:17.506308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 19:52:17.506315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 19:52:17.506421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 19:52:17.507115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 19:52:17.507822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 19:52:17.509076 140175996602176 registry.py:47] resolver HttpCompressedFileResolver does not support the provided handle.
I0920 19:52:17.509226 140175996602176 registry.py:47] resolver GcsCompressedFileResolver does not support the provided handle.
I0920 19:52:25.069205 140175996602176 eval_gan_lib.py:133] Generator inputs: {'z': <hub.ParsedTensorInfo shape=(64, 120) dtype=float32 is_sparse=False>, 'labels': <hub.ParsedTensorInfo shape=(64,) dtype=int32 is_sparse=False>}
INFO:tensorflow:Saver not created because there are no variables in the graph to restore
I0920 19:52:26.181346 140175996602176 saver.py:1499] Saver not created because there are no variables in the graph to restore
2020-09-20 19:52:27.468042: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
I0920 19:53:08.984992 140175996602176 eval_gan_lib.py:80] update_accu_switches: [<tf.Variable 'gen_module/generator/B1/bn1/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B1/bn2/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B2/bn1/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B2/bn2/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B3/bn1/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B3/bn2/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B4/bn1/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B4/bn2/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B5/bn1/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B5/bn2/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/final_norm/accu/update_accus:0' shape=() dtype=int32>]
I0920 19:53:09.505868 140175996602176 eval_gan_lib.py:88] Updating BN accumulators 0/3200 steps.
2020-09-20 19:53:19.725175: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 19:53:24.112048: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
I0920 19:55:01.345797 140175996602176 eval_gan_lib.py:88] Updating BN accumulators 500/3200 steps.
I0920 19:56:23.315846 140175996602176 eval_gan_lib.py:88] Updating BN accumulators 1000/3200 steps.
I0920 19:57:45.289260 140175996602176 eval_gan_lib.py:88] Updating BN accumulators 1500/3200 steps.
I0920 19:59:07.239946 140175996602176 eval_gan_lib.py:88] Updating BN accumulators 2000/3200 steps.
I0920 20:00:29.177574 140175996602176 eval_gan_lib.py:88] Updating BN accumulators 2500/3200 steps.
I0920 20:01:51.117966 140175996602176 eval_gan_lib.py:88] Updating BN accumulators 3000/3200 steps.
I0920 20:02:24.419314 140175996602176 eval_gan_lib.py:91] Done updating BN accumulators.
I0920 20:02:52.425838 140175996602176 eval_gan_lib.py:163] Exported generator with accumulated batch stats to gs://fewshot_gan/model_imagenet_aet_bce0/tfhub/45000/model-with-accu.ckpt.
I0920 20:02:52.426087 140175996602176 eval_gan_lib.py:168] Generating fake data set 1/3.
I0920 20:02:52.426142 140175996602176 eval_utils.py:146] Generating a fake data set.
0.0 1.0
I0920 20:05:17.118352 140175996602176 eval_utils.py:162] Done sampling a generated data set.
I0920 20:05:17.119542 140175996602176 eval_gan_lib.py:173] Computing inception features for generated data 1/3.
2020-09-20 20:05:17.121134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:05:17.121617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:05:17.121680: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:05:17.121694: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:05:17.121706: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:05:17.121718: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:05:17.121729: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:05:17.121740: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:05:17.121760: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:05:17.121819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:05:17.122261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:05:17.122670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:05:17.122766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:05:17.122774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:05:17.122781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:05:17.122876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:05:17.123408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:05:17.123832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow_gan/python/eval/classifier_metrics.py:136: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

W0920 20:05:17.171129 140175996602176 deprecation_wrapper.py:119] From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow_gan/python/eval/classifier_metrics.py:136: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.

2020-09-20 20:05:20.776163: W tensorflow/core/framework/op_def_util.cc:357] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().
WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
W0920 20:05:20.921069 140175996602176 deprecation.py:323] From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd845ef28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd845ef28>>: AttributeError: module 'gast' has no attribute 'Index'
W0920 20:05:20.942996 140175996602176 ag_logging.py:145] Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd845ef28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd845ef28>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bee108b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bee108b00>>: AttributeError: module 'gast' has no attribute 'Index'
W0920 20:05:20.960559 140175996602176 ag_logging.py:145] Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bee108b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bee108b00>>: AttributeError: module 'gast' has no attribute 'Index'
I0920 20:06:59.129462 140175996602176 eval_gan_lib.py:168] Generating fake data set 2/3.
I0920 20:06:59.129658 140175996602176 eval_utils.py:146] Generating a fake data set.
0.0 1.0
I0920 20:09:18.046596 140175996602176 eval_utils.py:162] Done sampling a generated data set.
I0920 20:09:18.047837 140175996602176 eval_gan_lib.py:173] Computing inception features for generated data 2/3.
2020-09-20 20:09:18.048883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:09:18.049441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:09:18.049508: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:09:18.049521: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:09:18.049531: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:09:18.049541: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:09:18.049551: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:09:18.049561: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:09:18.049572: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:09:18.049638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:09:18.050079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:09:18.050488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:09:18.050522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:09:18.050529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:09:18.050536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:09:18.050618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:09:18.051056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:09:18.051536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd071d828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd071d828>>: AttributeError: module 'gast' has no attribute 'Index'
W0920 20:09:20.346240 140175996602176 ag_logging.py:145] Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd071d828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd071d828>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd071d668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd071d668>>: AttributeError: module 'gast' has no attribute 'Index'
W0920 20:09:20.364142 140175996602176 ag_logging.py:145] Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd071d668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd071d668>>: AttributeError: module 'gast' has no attribute 'Index'
I0920 20:10:57.412744 140175996602176 eval_utils.py:68] Deleting references to images: (50000, 128, 128, 3)
I0920 20:10:57.431305 140175996602176 eval_gan_lib.py:168] Generating fake data set 3/3.
I0920 20:10:57.431439 140175996602176 eval_utils.py:146] Generating a fake data set.
0.0 1.0
I0920 20:13:16.353887 140175996602176 eval_utils.py:162] Done sampling a generated data set.
I0920 20:13:16.355125 140175996602176 eval_gan_lib.py:173] Computing inception features for generated data 3/3.
2020-09-20 20:13:16.356207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:13:16.356731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:13:16.356795: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:13:16.356807: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:13:16.356817: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:13:16.356827: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:13:16.356837: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:13:16.356846: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:13:16.356858: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:13:16.356926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:13:16.357386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:13:16.357804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:13:16.357854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:13:16.357861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:13:16.357868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:13:16.357956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:13:16.358427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:13:16.358844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd84b5470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd84b5470>>: AttributeError: module 'gast' has no attribute 'Index'
W0920 20:13:18.832204 140175996602176 ag_logging.py:145] Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd84b5470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd84b5470>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd84de0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd84de0b8>>: AttributeError: module 'gast' has no attribute 'Index'
W0920 20:13:18.850381 140175996602176 ag_logging.py:145] Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd84de0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd84de0b8>>: AttributeError: module 'gast' has no attribute 'Index'
I0920 20:14:55.945339 140175996602176 eval_utils.py:68] Deleting references to images: (50000, 128, 128, 3)
I0920 20:14:56.050843 140175996602176 eval_utils.py:109] Start loading real data.
W0920 20:14:56.052858 140175996602176 datasets.py:166] No TPUContext, using unmodified dataset seed 547.
I0920 20:14:56.053000 140175996602176 datasets.py:310] eval_input_fn(): params={} seed=547
I0920 20:14:56.440920 140175996602176 dataset_builder.py:157] Overwrite dataset info from restored data version.
I0920 20:14:57.538266 140175996602176 dataset_builder.py:193] Reusing dataset imagenet2012 (gs://tpu-comparegan/imagenet/imagenet2012/2.0.0)
WARNING:tensorflow:From /home/nupkumar/compare_gan/compare_gan/datasets.py:474: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

W0920 20:14:57.720898 140175996602176 deprecation_wrapper.py:119] From /home/nupkumar/compare_gan/compare_gan/datasets.py:474: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.

WARNING:tensorflow:From /home/nupkumar/compare_gan/compare_gan/eval_utils.py:114: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
W0920 20:14:57.729162 140175996602176 deprecation.py:323] From /home/nupkumar/compare_gan/compare_gan/eval_utils.py:114: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
2020-09-20 20:14:57.742221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:14:57.742764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:14:57.742829: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:14:57.742842: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:14:57.742852: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:14:57.742862: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:14:57.742871: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:14:57.742881: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:14:57.742891: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:14:57.742965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:14:57.743487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:14:57.743902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:14:57.743929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:14:57.743936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:14:57.743942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:14:57.744014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:14:57.744457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:14:57.744884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 20:18:07.420480 140175996602176 eval_utils.py:140] Done loading real data.
I0920 20:18:07.420773 140175996602176 eval_gan_lib.py:188] Getting Inception features for real images.
2020-09-20 20:18:07.421857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:18:07.422423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:18:07.422533: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:18:07.422548: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:18:07.422559: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:18:07.422569: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:18:07.422579: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:18:07.422589: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:18:07.422601: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:18:07.422673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:18:07.423176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:18:07.423637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:18:07.423675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:18:07.423683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:18:07.423691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:18:07.423789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:18:07.424261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:18:07.424692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd80d86d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd80d86d8>>: AttributeError: module 'gast' has no attribute 'Index'
W0920 20:18:09.726106 140175996602176 ag_logging.py:145] Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd80d86d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd80d86d8>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd80f50f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd80f50f0>>: AttributeError: module 'gast' has no attribute 'Index'
W0920 20:18:09.743629 140175996602176 ag_logging.py:145] Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd80f50f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd80f50f0>>: AttributeError: module 'gast' has no attribute 'Index'
I0920 20:19:47.254091 140175996602176 inception_score.py:41] Computing inception score.
2020-09-20 20:19:47.991041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:19:47.991573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:19:47.991639: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:19:47.991654: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:19:47.991665: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:19:47.991676: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:19:47.991686: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:19:47.991695: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:19:47.991706: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:19:47.991768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:19:47.992207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:19:47.992616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:19:47.992644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:19:47.992651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:19:47.992657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:19:47.992734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:19:47.993179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:19:47.993604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 20:19:56.648429 140175996602176 inception_score.py:47] Inception score: 54.875
I0920 20:19:56.649402 140175996602176 inception_score.py:41] Computing inception score.
2020-09-20 20:19:57.392557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:19:57.393060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:19:57.393128: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:19:57.393141: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:19:57.393150: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:19:57.393159: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:19:57.393168: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:19:57.393177: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:19:57.393187: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:19:57.393248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:19:57.393698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:19:57.394134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:19:57.394161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:19:57.394167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:19:57.394173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:19:57.394249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:19:57.394695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:19:57.395110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 20:20:05.932964 140175996602176 inception_score.py:47] Inception score: 54.533
I0920 20:20:05.933215 140175996602176 inception_score.py:41] Computing inception score.
2020-09-20 20:20:06.692650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:20:06.693196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:20:06.693255: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:20:06.693270: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:20:06.693283: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:20:06.693295: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:20:06.693317: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:20:06.693330: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:20:06.693343: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:20:06.693411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:20:06.693860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:20:06.694269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:20:06.694299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:20:06.694315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:20:06.694325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:20:06.694414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:20:06.694862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:20:06.695321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 20:20:15.294580 140175996602176 inception_score.py:47] Inception score: 55.085
I0920 20:20:15.295094 140175996602176 eval_gan_lib.py:209] Computed results for task <compare_gan.metrics.inception_score.InceptionScoreTask object at 0x7f7ca25c9d68>: {'inception_score_mean': 54.83086, 'inception_score_std': 0.22738533, 'inception_score_list': '54.875202_54.532867_55.084526'}
I0920 20:20:15.295202 140175996602176 fid_score.py:45] Calculating FID.
WARNING:tensorflow:From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow_gan/python/eval/classifier_metrics.py:102: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0920 20:20:18.272474 140175996602176 deprecation.py:323] From /home/nupkumar/venv_gan/lib/python3.6/site-packages/tensorflow_gan/python/eval/classifier_metrics.py:102: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2020-09-20 20:20:18.288073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:20:18.288622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:20:18.288683: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:20:18.288699: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:20:18.288711: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:20:18.288724: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:20:18.288736: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:20:18.288748: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:20:18.288762: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:20:18.288825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:20:18.289272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:20:18.289691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:20:18.289723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:20:18.289732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:20:18.289741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:20:18.289827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:20:18.290275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:20:18.290713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 20:21:43.166694 140175996602176 fid_score.py:54] Frechet Inception Distance: 14.560.
I0920 20:21:43.166983 140175996602176 fid_score.py:45] Calculating FID.
2020-09-20 20:21:46.217214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:21:46.217758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:21:46.217820: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:21:46.217833: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:21:46.217866: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:21:46.217876: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:21:46.217885: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:21:46.217894: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:21:46.217905: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:21:46.217962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:21:46.218411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:21:46.218811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:21:46.218891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:21:46.218898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:21:46.218904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:21:46.218991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:21:46.219502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:21:46.219946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 20:23:13.283041 140175996602176 fid_score.py:54] Frechet Inception Distance: 14.756.
I0920 20:23:13.283422 140175996602176 fid_score.py:45] Calculating FID.
2020-09-20 20:23:16.370718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:23:16.371322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:23:16.371403: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:23:16.371419: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:23:16.371430: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:23:16.371442: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:23:16.371454: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:23:16.371464: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:23:16.371477: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:23:16.371564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:23:16.372053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:23:16.372495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:23:16.372524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:23:16.372531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:23:16.372537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:23:16.372653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:23:16.373088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:23:16.373511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 20:24:45.990774 140175996602176 fid_score.py:54] Frechet Inception Distance: 14.745.
I0920 20:24:45.991526 140175996602176 eval_gan_lib.py:209] Computed results for task <compare_gan.metrics.fid_score.FIDScoreTask object at 0x7f7ca256d8d0>: {'fid_score_mean': 14.687005, 'fid_score_std': 0.089927234, 'fid_score_list': '14.559978_14.755835_14.745205'}
I0920 20:24:46.035843 140175996602176 runner_lib.py:287] Evaluation result for checkpoint gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-45000: {'inception_score_mean': 54.83086, 'inception_score_std': 0.22738533, 'inception_score_list': '54.875202_54.532867_55.084526', 'fid_score_mean': 14.687005, 'fid_score_std': 0.089927234, 'fid_score_list': '14.559978_14.755835_14.745205'} (default value: -1.0)
WARNING:tensorflow:From /home/nupkumar/compare_gan/compare_gan/runner_lib.py:205: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.

W0920 20:24:46.036115 140175996602176 deprecation_wrapper.py:119] From /home/nupkumar/compare_gan/compare_gan/runner_lib.py:205: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.

2020-09-20 20:24:54.061328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:24:54.061879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:24:54.061946: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:24:54.061966: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:24:54.061977: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:24:54.061987: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:24:54.061998: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:24:54.062008: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:24:54.062019: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:24:54.062085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:24:54.062570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:24:54.062988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:24:54.063020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:24:54.063028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:24:54.063034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:24:54.063137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:24:54.063666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:24:54.064103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 20:25:49.639002 140175996602176 native_module.py:404] Exported TF-Hub module to: gs://fewshot_gan/model_imagenet_aet_bce0/tfhub/50000
I0920 20:25:49.686341 140175996602176 datasets.py:103] ImageDatasetV2(name=imagenet_128, tfds_name=imagenet2012, resolution=128, colors=3, num_classes=1000, eval_test_samples=50000, seed=547)
2020-09-20 20:25:49.688482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:25:49.689006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:25:49.689065: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:25:49.689078: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:25:49.689088: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:25:49.689101: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:25:49.689111: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:25:49.689137: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:25:49.689148: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:25:49.689225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:25:49.689676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:25:49.690079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:25:49.690106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:25:49.690126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:25:49.690132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:25:49.690211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:25:49.690661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:25:49.691076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 20:25:49.691271 140175996602176 registry.py:47] resolver HttpCompressedFileResolver does not support the provided handle.
I0920 20:25:49.691400 140175996602176 registry.py:47] resolver GcsCompressedFileResolver does not support the provided handle.
I0920 20:25:55.774586 140175996602176 eval_gan_lib.py:133] Generator inputs: {'z': <hub.ParsedTensorInfo shape=(64, 120) dtype=float32 is_sparse=False>, 'labels': <hub.ParsedTensorInfo shape=(64,) dtype=int32 is_sparse=False>}
INFO:tensorflow:Saver not created because there are no variables in the graph to restore
I0920 20:25:56.830627 140175996602176 saver.py:1499] Saver not created because there are no variables in the graph to restore
I0920 20:26:13.460888 140175996602176 eval_gan_lib.py:80] update_accu_switches: [<tf.Variable 'gen_module/generator/B1/bn1/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B1/bn2/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B2/bn1/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B2/bn2/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B3/bn1/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B3/bn2/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B4/bn1/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B4/bn2/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B5/bn1/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/B5/bn2/accu/update_accus:0' shape=() dtype=int32>, <tf.Variable 'gen_module/generator/final_norm/accu/update_accus:0' shape=() dtype=int32>]
I0920 20:26:14.149593 140175996602176 eval_gan_lib.py:88] Updating BN accumulators 0/3200 steps.
I0920 20:27:46.839663 140175996602176 eval_gan_lib.py:88] Updating BN accumulators 500/3200 steps.
I0920 20:29:09.032177 140175996602176 eval_gan_lib.py:88] Updating BN accumulators 1000/3200 steps.
I0920 20:30:31.170536 140175996602176 eval_gan_lib.py:88] Updating BN accumulators 1500/3200 steps.
I0920 20:31:53.371019 140175996602176 eval_gan_lib.py:88] Updating BN accumulators 2000/3200 steps.
I0920 20:33:15.416169 140175996602176 eval_gan_lib.py:88] Updating BN accumulators 2500/3200 steps.
I0920 20:34:37.363803 140175996602176 eval_gan_lib.py:88] Updating BN accumulators 3000/3200 steps.
I0920 20:35:10.685260 140175996602176 eval_gan_lib.py:91] Done updating BN accumulators.
I0920 20:35:38.418316 140175996602176 eval_gan_lib.py:163] Exported generator with accumulated batch stats to gs://fewshot_gan/model_imagenet_aet_bce0/tfhub/50000/model-with-accu.ckpt.
I0920 20:35:38.418554 140175996602176 eval_gan_lib.py:168] Generating fake data set 1/3.
I0920 20:35:38.418611 140175996602176 eval_utils.py:146] Generating a fake data set.
0.0 1.0
I0920 20:37:57.270547 140175996602176 eval_utils.py:162] Done sampling a generated data set.
I0920 20:37:57.271740 140175996602176 eval_gan_lib.py:173] Computing inception features for generated data 1/3.
2020-09-20 20:37:57.272773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:37:57.273293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:37:57.273364: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:37:57.273377: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:37:57.273387: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:37:57.273396: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:37:57.273405: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:37:57.273414: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:37:57.273425: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:37:57.273499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:37:57.273941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:37:57.274340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:37:57.274432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:37:57.274440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:37:57.274447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:37:57.274544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:37:57.274991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:37:57.275437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bee459278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bee459278>>: AttributeError: module 'gast' has no attribute 'Index'
W0920 20:37:59.604681 140175996602176 ag_logging.py:145] Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bee459278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bee459278>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bee4780b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bee4780b8>>: AttributeError: module 'gast' has no attribute 'Index'
W0920 20:37:59.621584 140175996602176 ag_logging.py:145] Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bee4780b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bee4780b8>>: AttributeError: module 'gast' has no attribute 'Index'
I0920 20:39:37.016973 140175996602176 eval_gan_lib.py:168] Generating fake data set 2/3.
I0920 20:39:37.017181 140175996602176 eval_utils.py:146] Generating a fake data set.
0.0 1.0
I0920 20:41:55.844495 140175996602176 eval_utils.py:162] Done sampling a generated data set.
I0920 20:41:55.845709 140175996602176 eval_gan_lib.py:173] Computing inception features for generated data 2/3.
2020-09-20 20:41:55.846926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:41:55.847510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:41:55.847595: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:41:55.847612: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:41:55.847627: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:41:55.847642: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:41:55.847657: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:41:55.847672: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:41:55.847689: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:41:55.847772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:41:55.848226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:41:55.848646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:41:55.848680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:41:55.848690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:41:55.848700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:41:55.848793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:41:55.849242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:41:55.849705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd823e518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd823e518>>: AttributeError: module 'gast' has no attribute 'Index'
W0920 20:41:58.122709 140175996602176 ag_logging.py:145] Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd823e518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd823e518>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd823e358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd823e358>>: AttributeError: module 'gast' has no attribute 'Index'
W0920 20:41:58.139634 140175996602176 ag_logging.py:145] Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd823e358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd823e358>>: AttributeError: module 'gast' has no attribute 'Index'
I0920 20:43:35.414479 140175996602176 eval_utils.py:68] Deleting references to images: (50000, 128, 128, 3)
I0920 20:43:35.433211 140175996602176 eval_gan_lib.py:168] Generating fake data set 3/3.
I0920 20:43:35.433351 140175996602176 eval_utils.py:146] Generating a fake data set.
0.0 1.0
I0920 20:45:54.274886 140175996602176 eval_utils.py:162] Done sampling a generated data set.
I0920 20:45:54.276199 140175996602176 eval_gan_lib.py:173] Computing inception features for generated data 3/3.
2020-09-20 20:45:54.277265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:45:54.277812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:45:54.277878: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:45:54.277891: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:45:54.277901: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:45:54.277925: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:45:54.277935: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:45:54.277956: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:45:54.277968: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:45:54.278039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:45:54.278501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:45:54.278905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:45:54.278989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:45:54.278997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:45:54.279003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:45:54.279111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:45:54.279632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:45:54.280071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd0263f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd0263f60>>: AttributeError: module 'gast' has no attribute 'Index'
W0920 20:45:56.602369 140175996602176 ag_logging.py:145] Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd0263f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd0263f60>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd028cc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd028cc88>>: AttributeError: module 'gast' has no attribute 'Index'
W0920 20:45:56.619184 140175996602176 ag_logging.py:145] Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd028cc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7bd028cc88>>: AttributeError: module 'gast' has no attribute 'Index'
I0920 20:47:34.094738 140175996602176 eval_utils.py:68] Deleting references to images: (50000, 128, 128, 3)
I0920 20:47:34.193761 140175996602176 eval_utils.py:109] Start loading real data.
W0920 20:47:34.195158 140175996602176 datasets.py:166] No TPUContext, using unmodified dataset seed 547.
I0920 20:47:34.195320 140175996602176 datasets.py:310] eval_input_fn(): params={} seed=547
I0920 20:47:34.565025 140175996602176 dataset_builder.py:157] Overwrite dataset info from restored data version.
I0920 20:47:35.610404 140175996602176 dataset_builder.py:193] Reusing dataset imagenet2012 (gs://tpu-comparegan/imagenet/imagenet2012/2.0.0)
2020-09-20 20:47:35.796624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:47:35.797152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:47:35.797232: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:47:35.797245: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:47:35.797255: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:47:35.797266: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:47:35.797275: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:47:35.797285: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:47:35.797296: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:47:35.797361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:47:35.797838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:47:35.798251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:47:35.798277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:47:35.798284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:47:35.798290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:47:35.798366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:47:35.798811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:47:35.799224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 20:50:45.359598 140175996602176 eval_utils.py:140] Done loading real data.
I0920 20:50:45.359862 140175996602176 eval_gan_lib.py:188] Getting Inception features for real images.
2020-09-20 20:50:45.361535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:50:45.362105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:50:45.362189: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:50:45.362202: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:50:45.362211: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:50:45.362221: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:50:45.362230: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:50:45.362239: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:50:45.362250: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:50:45.362340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:50:45.362818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:50:45.363286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:50:45.363325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:50:45.363333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:50:45.363339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:50:45.363464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:50:45.363939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:50:45.364382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7ba4391d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7ba4391d30>>: AttributeError: module 'gast' has no attribute 'Index'
W0920 20:50:47.670442 140175996602176 ag_logging.py:145] Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7ba4391d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7ba4391d30>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7ba4391b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7ba4391b70>>: AttributeError: module 'gast' has no attribute 'Index'
W0920 20:50:47.687291 140175996602176 ag_logging.py:145] Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7ba4391b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f7ba4391b70>>: AttributeError: module 'gast' has no attribute 'Index'
I0920 20:52:24.482101 140175996602176 inception_score.py:41] Computing inception score.
2020-09-20 20:52:25.232589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:25.233129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:52:25.233193: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:52:25.233208: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:52:25.233221: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:52:25.233234: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:52:25.233247: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:52:25.233260: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:52:25.233273: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:52:25.233351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:25.233804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:25.234213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:52:25.234246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:52:25.234256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:52:25.234265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:52:25.234356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:25.234809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:25.235259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 20:52:33.824354 140175996602176 inception_score.py:47] Inception score: 66.237
I0920 20:52:33.824601 140175996602176 inception_score.py:41] Computing inception score.
2020-09-20 20:52:34.563195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:34.563718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:52:34.564526: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:52:34.564542: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:52:34.564552: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:52:34.564563: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:52:34.564572: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:52:34.564582: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:52:34.564593: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:52:34.564672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:34.565158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:34.565575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:52:34.565603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:52:34.565610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:52:34.565617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:52:34.565695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:34.566158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:34.566598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 20:52:43.153799 140175996602176 inception_score.py:47] Inception score: 65.923
I0920 20:52:43.154058 140175996602176 inception_score.py:41] Computing inception score.
2020-09-20 20:52:43.895014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:43.895594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:52:43.895660: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:52:43.895675: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:52:43.895689: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:52:43.895714: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:52:43.895726: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:52:43.895739: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:52:43.895753: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:52:43.895823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:43.896277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:43.896699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:52:43.896733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:52:43.896743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:52:43.896751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:52:43.896844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:43.897294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:43.897730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 20:52:52.518120 140175996602176 inception_score.py:47] Inception score: 66.158
I0920 20:52:52.518639 140175996602176 eval_gan_lib.py:209] Computed results for task <compare_gan.metrics.inception_score.InceptionScoreTask object at 0x7f7ca25c9d68>: {'inception_score_mean': 66.1059, 'inception_score_std': 0.1331433, 'inception_score_list': '66.236916_65.92328_66.15753'}
I0920 20:52:52.518742 140175996602176 fid_score.py:45] Calculating FID.
2020-09-20 20:52:55.502618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:55.503218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:52:55.503301: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:52:55.503318: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:52:55.503337: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:52:55.503350: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:52:55.503362: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:52:55.503375: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:52:55.503388: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:52:55.503487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:55.504010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:55.504457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:52:55.504489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:52:55.504498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:52:55.504506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:52:55.504594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:55.505044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:52:55.505481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 20:54:19.421972 140175996602176 fid_score.py:54] Frechet Inception Distance: 11.383.
I0920 20:54:19.422251 140175996602176 fid_score.py:45] Calculating FID.
2020-09-20 20:54:22.405489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:54:22.406000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:54:22.406071: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:54:22.406086: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:54:22.406098: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:54:22.406110: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:54:22.406122: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:54:22.406133: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:54:22.406146: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:54:22.406209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:54:22.406650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:54:22.407061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:54:22.407091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:54:22.407098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:54:22.407115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:54:22.407204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:54:22.407667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:54:22.408091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 20:55:45.660063 140175996602176 fid_score.py:54] Frechet Inception Distance: 11.519.
I0920 20:55:45.660371 140175996602176 fid_score.py:45] Calculating FID.
2020-09-20 20:55:48.702499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:55:48.703025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:04.0
2020-09-20 20:55:48.703108: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2020-09-20 20:55:48.703123: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2020-09-20 20:55:48.703135: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2020-09-20 20:55:48.703147: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2020-09-20 20:55:48.703158: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2020-09-20 20:55:48.703170: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2020-09-20 20:55:48.703182: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-20 20:55:48.703261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:55:48.703726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:55:48.704190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-09-20 20:55:48.704219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-20 20:55:48.704226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-20 20:55:48.704232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2020-09-20 20:55:48.704309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:55:48.704762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-20 20:55:48.705178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15059 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)
I0920 20:57:11.792386 140175996602176 fid_score.py:54] Frechet Inception Distance: 11.396.
I0920 20:57:11.793031 140175996602176 eval_gan_lib.py:209] Computed results for task <compare_gan.metrics.fid_score.FIDScoreTask object at 0x7f7ca256d8d0>: {'fid_score_mean': 11.43262, 'fid_score_std': 0.061215457, 'fid_score_list': '11.382971_11.518863_11.396026'}
I0920 20:57:11.835511 140175996602176 runner_lib.py:287] Evaluation result for checkpoint gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-50000: {'inception_score_mean': 66.1059, 'inception_score_std': 0.1331433, 'inception_score_list': '66.236916_65.92328_66.15753', 'fid_score_mean': 11.43262, 'fid_score_std': 0.061215457, 'fid_score_list': '11.382971_11.518863_11.396026'} (default value: -1.0)
I0920 20:57:14.528524 140175996602176 runner_lib.py:177] Found checkpoints: {'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-45000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-25000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-15000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-30000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-0', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-20000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-40000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-50000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-10000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-35000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-5000'}
Evaluated checkpoints: {'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-45000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-25000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-15000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-30000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-40000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-20000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-50000', 'gs://fewshot_gan/model_imagenet_aet_bce0/model.ckpt-35000'}
Unevaluated checkpoints: []
I0920 20:57:15.284993 140175996602176 main.py:129] I"m done with my work, ciao!
